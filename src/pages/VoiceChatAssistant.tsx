import React, { useState, useRef, useEffect, useCallback } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { ScrollArea } from '@/components/ui/scroll-area';
import { useToast } from '@/components/ui/use-toast';
import { 
  Mic, 
  MicOff, 
  Send, 
  Volume2, 
  VolumeX, 
  Bot, 
  User,
  Trash2,
  AlertCircle
} from 'lucide-react';
import { cn } from '@/lib/utils';
import { supabase } from '@/integrations/supabase/client';
import { useAuth } from '@/hooks/useAuth';
import { getOpenAIKey } from '@/utils/getAPIKeys';

interface Message {
  id: string;
  type: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  isVoice?: boolean;
  thinking?: boolean;
}

interface VoiceState {
  isListening: boolean;
  isSpeaking: boolean;
  isConnected: boolean;
}

const VoiceChatAssistant = () => {
  const { user } = useAuth();
  const { toast } = useToast();
  const [userVoiceSettings, setUserVoiceSettings] = useState<any>(null);
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      type: 'assistant',
      content: '–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à –≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è. –ú–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö, —Å–º–µ—Ç–∞—Ö, –∑–∞–¥–∞—á–∞—Ö –∏ –ø–æ–º–æ—á—å —É–ø—Ä–∞–≤–ª—è—Ç—å –¥—Ä—É–≥–∏–º–∏ –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫–∞–º–∏. –°–ø—Ä–∞—à–∏–≤–∞–π—Ç–µ –≥–æ–ª–æ—Å–æ–º –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–º!',
      timestamp: new Date(),
    }
  ]);
  const [inputValue, setInputValue] = useState('');
  const [voiceState, setVoiceState] = useState<VoiceState>({
    isListening: false,
    isSpeaking: false,
    isConnected: false
  });
  const [isVoiceMode, setIsVoiceMode] = useState(false);
  const [browserSupport, setBrowserSupport] = useState({
    mediaDevices: false,
    speechSynthesis: false,
    mediaRecorder: false
  });
  
  const messagesEndRef = useRef<HTMLDivElement>(null);

  // Load user voice settings
  useEffect(() => {
    const loadVoiceSettings = async () => {
      if (!user) return;
      
      try {
        const { data, error } = await supabase
          .from('profiles')
          .select('voice_settings')
          .eq('user_id', user.id)
          .single();
          
        if (error) {
          console.warn('Error loading voice settings from profiles:', error);
          // Fallback to AI assistant settings
          const { data: aiData, error: aiError } = await supabase
            .from('ai_assistant_settings')
            .select('settings')
            .eq('user_id', user.id)
            .eq('assistant_type', 'voice_assistant')
            .maybeSingle();
            
          if (!aiError && aiData?.settings) {
            console.log('Loaded voice settings from AI assistant settings:', aiData.settings);
            setUserVoiceSettings(aiData.settings);
          }
        } else {
          console.log('Loaded voice settings from profile:', data?.voice_settings);
          setUserVoiceSettings(data?.voice_settings);
        }
      } catch (error) {
        console.error('Error loading voice settings:', error);
      }
    };
    
    loadVoiceSettings();
  }, [user]);

  // Check browser capabilities
  useEffect(() => {
    const checkBrowserSupport = () => {
      const hasWebkitSpeech = !!(window as any).webkitSpeechRecognition;
      const hasSpeechRecognition = !!(window as any).SpeechRecognition;
      
      setBrowserSupport({
        mediaDevices: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia),
        speechSynthesis: !!window.speechSynthesis,
        mediaRecorder: !!window.MediaRecorder && (hasWebkitSpeech || hasSpeechRecognition)
      });
    };
    
    checkBrowserSupport();
  }, []);

  // Scroll to bottom when new messages arrive
  const scrollToBottom = useCallback(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, []);

  useEffect(() => {
    scrollToBottom();
  }, [messages, scrollToBottom]);

  // Add message helper
  const addMessage = useCallback((type: 'user' | 'assistant', content: string, isVoice = false) => {
    const newMessage: Message = {
      id: Date.now().toString(),
      type,
      content,
      timestamp: new Date(),
      isVoice
    };
    setMessages(prev => [...prev, newMessage]);
  }, []);

  // Toggle voice mode
  const toggleVoiceMode = () => {
    const newMode = !isVoiceMode;
    setIsVoiceMode(newMode);
    
    if (newMode) {
      toast({
        title: '–ì–æ–ª–æ—Å–æ–≤–æ–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω',
        description: '–¢–µ–ø–µ—Ä—å –ø–æ–º–æ—â–Ω–∏–∫ –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–º'
      });
    } else {
      // Stop any current speech
      if (window.speechSynthesis.speaking) {
        window.speechSynthesis.cancel();
      }
      setVoiceState(prev => ({ ...prev, isSpeaking: false }));
    }
  };

  // Clear chat
  const clearChat = () => {
    setMessages([{
      id: '1',
      type: 'assistant',
      content: '–ß–∞—Ç –æ—á–∏—â–µ–Ω. –ö–∞–∫ –º–æ–≥—É –ø–æ–º–æ—á—å?',
      timestamp: new Date(),
    }]);
  };

  // Send text message
  const handleSendMessage = useCallback(async () => {
    if (!inputValue.trim()) return;

    const userMessage = inputValue.trim();
    setInputValue('');
    
    addMessage('user', userMessage);

    // Add thinking indicator
    const thinkingMessage: Message = {
      id: 'thinking',
      type: 'assistant',
      content: '',
      timestamp: new Date(),
      thinking: true
    };
    setMessages(prev => [...prev, thinkingMessage]);

    // Get AI response
    try {
      const response = await generateResponse(userMessage);
      
      // –£–±–∏—Ä–∞–µ–º thinking –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ streaming (–ø—Ä–∏ streaming —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ)
      setMessages(prev => {
        const hasStreamingResponse = prev.some(m => m.id !== 'thinking' && m.type === 'assistant' && m.timestamp.getTime() > thinkingMessage.timestamp.getTime());
        if (hasStreamingResponse) {
          // –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å streaming –æ—Ç–≤–µ—Ç, —Ç–æ–ª—å–∫–æ —É–±–∏—Ä–∞–µ–º thinking
          return prev.filter(m => m.id !== 'thinking');
        } else {
          // –ï—Å–ª–∏ –Ω–µ—Ç streaming –æ—Ç–≤–µ—Ç–∞, —É–±–∏—Ä–∞–µ–º thinking –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
          return prev.filter(m => m.id !== 'thinking').concat([{
            id: Date.now().toString(),
            type: 'assistant',
            content: response,
            timestamp: new Date()
          }]);
        }
      });
      
      // If voice mode is enabled, speak the response
      if (isVoiceMode) {
        await speakResponse(response);
      }
    } catch (error) {
      setMessages(prev => prev.filter(m => m.id !== 'thinking'));
      addMessage('assistant', '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
    }
  }, [inputValue, addMessage, isVoiceMode, browserSupport.speechSynthesis]);

  // Generate AI response with streaming support
  const generateResponse = async (userMessage: string): Promise<string> => {
    try {
      console.log('Calling enhanced-voice-chat edge function...');
      
      // –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
      const { data: { session } } = await supabase.auth.getSession();
      if (!session) {
        throw new Error('User not authenticated');
      }

      // –î–µ–ª–∞–µ–º –ø—Ä—è–º–æ–π fetch –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ streaming
      const response = await fetch(
        `https://nxyzmxqtzsvjezmkmkja.supabase.co/functions/v1/enhanced-voice-chat`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${session.access_token}`,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            message: userMessage,
            conversation_history: messages.slice(-10).map(m => ({
              role: m.type,
              content: m.content
            }))
          })
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`HTTP ${response.status}: ${errorText}`);
      }

      // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ç–æ–∫–æ–≤—ã–π –ª–∏ —ç—Ç–æ –æ—Ç–≤–µ—Ç
      const contentType = response.headers.get('content-type');
      
      if (contentType?.includes('text/plain')) {
        // –ü–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç
        console.log('Processing streaming response...');
        return await handleStreamingResponse(response);
      } else {
        // –û–±—ã—á–Ω—ã–π JSON –æ—Ç–≤–µ—Ç
        const data = await response.json();
        console.log('Response from edge function:', data);
        return data?.response || '–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –ø–æ–º–æ—â–Ω–∏–∫–∞';
      }
      
    } catch (error) {
      console.error('Error calling enhanced-voice-chat:', error);
      return '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API –∏–ª–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.';
    }
  };

  // –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
  const handleStreamingResponse = async (response: Response): Promise<string> => {
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body');
    }

    const decoder = new TextDecoder();
    let fullResponse = '';
    let currentMessageId = '';
    const isStreamingMode = userVoiceSettings?.streaming_enabled;

    try {
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) {
          break;
        }

        const chunk = decoder.decode(value, { stream: true });
        const lines = chunk.split('\n').filter(line => line.trim());

        for (const line of lines) {
          try {
            const parsed = JSON.parse(line);
            
            if (parsed.type === 'content') {
              fullResponse += parsed.content;
              
              // –ü—Ä–∏ –≤–∫–ª—é—á–µ–Ω–Ω–æ–π –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ —Å—Ä–∞–∑—É –ø—Ä–æ–∏–∑–Ω–æ—Å–∏–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≥–æ–ª–æ—Å–æ–º
              if (isStreamingMode && isVoiceMode) {
                // –ü—Ä–æ–∏–∑–Ω–æ—Å–∏–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
                const newContent = parsed.content;
                if (newContent && newContent.trim()) {
                  speakResponse(newContent);
                }
              } else {
                // –û–±—ã—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ - –æ–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç–µ
                if (!currentMessageId) {
                  currentMessageId = Date.now().toString();
                  setMessages(prev => [...prev, {
                    id: currentMessageId,
                    type: 'assistant',
                    content: parsed.content,
                    timestamp: new Date()
                  }]);
                } else {
                  setMessages(prev => 
                    prev.map(msg => 
                      msg.id === currentMessageId 
                        ? { ...msg, content: fullResponse }
                        : msg
                    )
                  );
                }
              }
            } else if (parsed.type === 'done') {
              console.log('Streaming completed');
              // –í –ø–æ—Ç–æ–∫–æ–≤–æ–º –≥–æ–ª–æ—Å–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ —á–∞—Ç - —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ—Å
              // –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ —á–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï –≤–∫–ª—é—á–µ–Ω–∞ –ø–æ—Ç–æ–∫–æ–≤–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –∏–ª–∏ –ù–ï –≥–æ–ª–æ—Å–æ–≤–æ–π —Ä–µ–∂–∏–º
              if (!(isStreamingMode && isVoiceMode) && fullResponse) {
                setMessages(prev => [...prev, {
                  id: Date.now().toString(),
                  type: 'assistant',
                  content: fullResponse,
                  timestamp: new Date()
                }]);
              }
            }
          } catch (e) {
            console.warn('Failed to parse streaming chunk:', e);
          }
        }
      }

      return fullResponse;
      
    } finally {
      reader.releaseLock();
    }
  };

  // Enhanced TTS with fallback to server
  const speakResponse = async (text: string) => {
    if (!isVoiceMode) return;

    // Check user TTS provider settings
    const ttsProvider = userVoiceSettings?.tts_provider || 'openai';
    const useBrowserTTS = (userVoiceSettings?.voice_provider === 'web_speech') || ttsProvider === 'web_speech';
    
    // Use browser TTS when explicitly selected via voice_provider or tts_provider
    if (useBrowserTTS && window.speechSynthesis) {
      try {
        speechSynthesis.cancel();

        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'ru-RU';
        utterance.rate = userVoiceSettings?.speech_rate ?? 1.0;
        utterance.pitch = userVoiceSettings?.speech_pitch ?? 1.0;

        utterance.onstart = () => {
          setVoiceState(prev => ({ ...prev, isSpeaking: true }));
        };

        utterance.onend = () => {
          setVoiceState(prev => ({ ...prev, isSpeaking: false }));
        };

        utterance.onerror = (error) => {
          console.error('Browser TTS error:', error);
          setVoiceState(prev => ({ ...prev, isSpeaking: false }));
          // Fallback to server TTS on error
          handleServerTTS(text);
        };

        speechSynthesis.speak(utterance);
        return;
      } catch (error) {
        console.error('Browser TTS error:', error);
      }
    }

    // Use server TTS for OpenAI, ElevenLabs, Yandex or as fallback
    handleServerTTS(text);
  };

  // Server-based text-to-speech fallback
  const handleServerTTS = async (text: string) => {
    if (!user) {
      toast({
        title: '–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏',
        description: '–í–æ–π–¥–∏—Ç–µ –≤ —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π',
        variant: 'destructive'
      });
      return;
    }

    // –ù–µ —Ç—Ä–µ–±—É–µ–º –∫–ª—é—á –∑–∞—Ä–∞–Ω–µ–µ ‚Äî –æ–Ω –±—É–¥–µ—Ç –ø–æ–ª—É—á–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ –∏–∑ –ë–î
    try {
      setVoiceState(prev => ({ ...prev, isSpeaking: true }));

      // –ü–æ–ª—É—á–∞–µ–º –≥–æ–ª–æ—Å–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
      const ttsProvider = userVoiceSettings?.tts_provider || 'openai';
      const voiceId = userVoiceSettings?.voice_id || 'alloy';

      // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è TTS (–º–∞–∫—Å–∏–º—É–º 4000 —Å–∏–º–≤–æ–ª–æ–≤)
      const truncatedText = text.length > 4000 ? text.substring(0, 4000) + "..." : text;

      console.log('Voice settings for TTS:', { 
        ttsProvider, 
        voiceId, 
        textLength: truncatedText.length,
        userVoiceSettings 
      });

      const response = await supabase.functions.invoke('text-to-speech', {
        body: { 
          text: truncatedText,
          provider: ttsProvider,
          voice: voiceId,
          rate: userVoiceSettings?.speech_rate ?? 1,
          pitch: userVoiceSettings?.speech_pitch ?? 1
        }
      });

      if (response.error) {
        console.error('TTS Response error:', response.error);
        throw new Error(`TTS API error: ${response.error.message || 'Unknown error'}`);
      }

      if (!response.data) {
        console.error('TTS No data received:', response);
        throw new Error('No data received from TTS service');
      }

      const { audioContent } = response.data;
      if (audioContent) {
        // Play base64 audio
        const audio = new Audio(`data:audio/mp3;base64,${audioContent}`);
        
        audio.onended = () => {
          setVoiceState(prev => ({ ...prev, isSpeaking: false }));
        };

        audio.onerror = () => {
          setVoiceState(prev => ({ ...prev, isSpeaking: false }));
          toast({
            title: '–û—à–∏–±–∫–∞ –æ–∑–≤—É—á–∫–∏',
            description: '–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –∞—É–¥–∏–æ',
            variant: 'destructive'
          });
        };

        await audio.play();
      }
    } catch (error) {
      console.error('Server TTS error:', error);
      setVoiceState(prev => ({ ...prev, isSpeaking: false }));
      toast({
        title: '–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏',
        description: '–ì–æ–ª–æ—Å–æ–≤–æ–π –æ—Ç–≤–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
        variant: 'destructive'
      });
    }
  };

  // Voice input with fallback to server STT
  const handleVoiceInput = async () => {
    if (!browserSupport.mediaDevices) {
      toast({
        title: '–ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
        description: '–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥',
        variant: 'destructive'
      });
      return;
    }

    if (voiceState.isListening) {
      // Stop listening
      setVoiceState(prev => ({ ...prev, isListening: false }));
      toast({
        title: '–ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞',
        description: '–ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω'
      });
      return;
    }

    // Try Web Speech API first
    const hasWebSpeech = !!(window as any).webkitSpeechRecognition || !!(window as any).SpeechRecognition;
    
    if (hasWebSpeech) {
      try {
        setVoiceState(prev => ({ ...prev, isListening: true }));
        
        const recognition = new (window as any).webkitSpeechRecognition() || new (window as any).SpeechRecognition();
        recognition.lang = 'ru-RU';
        recognition.continuous = false;
        recognition.interimResults = false;

        recognition.onresult = (event: any) => {
          const transcript = event.results[0][0].transcript;
          setInputValue(transcript);
          setVoiceState(prev => ({ ...prev, isListening: false }));
          
          toast({
            title: '–ì–æ–ª–æ—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω',
            description: `–¢–µ–∫—Å—Ç: "${transcript}"`
          });

          // Auto-send in voice mode
          setTimeout(() => {
            if (transcript.trim()) {
              // Directly send the transcript without relying on inputValue state
              const userMessage = transcript.trim();
              setInputValue('');
              
              addMessage('user', userMessage, true);

              // Add thinking indicator
              const thinkingMessage: Message = {
                id: 'thinking',
                type: 'assistant',
                content: '',
                timestamp: new Date(),
                thinking: true
              };
              setMessages(prev => [...prev, thinkingMessage]);

              // Get AI response
              generateResponse(userMessage).then(async (response) => {
                setMessages(prev => {
                  const hasStreamingResponse = prev.some(m => m.id !== 'thinking' && m.type === 'assistant' && m.timestamp.getTime() > thinkingMessage.timestamp.getTime());
                  if (hasStreamingResponse) {
                    return prev.filter(m => m.id !== 'thinking');
                  } else {
                    return prev.filter(m => m.id !== 'thinking').concat([{
                      id: Date.now().toString(),
                      type: 'assistant',
                      content: response,
                      timestamp: new Date()
                    }]);
                  }
                });
                
                if (isVoiceMode) {
                  await speakResponse(response);
                }
              }).catch(() => {
                setMessages(prev => prev.filter(m => m.id !== 'thinking'));
                addMessage('assistant', '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
              });
            }
          }, 500);
        };

        recognition.onerror = (event: any) => {
          console.error('Speech recognition error:', event.error);
          setVoiceState(prev => ({ ...prev, isListening: false }));
          
          // Fallback to server STT on error
          handleServerSTT();
        };

        recognition.onend = () => {
          setVoiceState(prev => ({ ...prev, isListening: false }));
        };

        recognition.start();
        
        toast({
          title: '–°–ª—É—à–∞—é...',
          description: '–ì–æ–≤–æ—Ä–∏—Ç–µ —á–µ—Ç–∫–æ –∏ –º–µ–¥–ª–µ–Ω–Ω–æ'
        });

      } catch (error) {
        console.error('Error starting Web Speech:', error);
        handleServerSTT(); // Fallback to server STT
      }
    } else {
      handleServerSTT(); // Use server STT if Web Speech not available
    }
  };

  // Server-based speech-to-text fallback
  const handleServerSTT = async () => {
    if (!user) {
      toast({
        title: '–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏',
        description: '–í–æ–π–¥–∏—Ç–µ –≤ —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π',
        variant: 'destructive'
      });
      return;
    }

    const openaiKey = await getOpenAIKey(user.id);
    if (!openaiKey) {
      toast({
        title: 'API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω',
        description: '–ù–∞—Å—Ç—Ä–æ–π—Ç–µ OpenAI API –∫–ª—é—á –≤ —Ä–∞–∑–¥–µ–ª–µ "–ù–∞—Å—Ç—Ä–æ–π–∫–∏" ‚Üí "API –ö–ª—é—á–∏"',
        variant: 'destructive'
      });
      return;
    }
    let mediaRecorder: MediaRecorder | null = null;
    let chunks: Blob[] = [];

    try {
      setVoiceState(prev => ({ ...prev, isListening: true }));
      
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(chunks, { type: 'audio/webm' });
        chunks = [];
        
        try {
          // Convert to base64
          const reader = new FileReader();
          reader.onload = async () => {
            const base64Audio = (reader.result as string).split(',')[1];
            
            // Send to server STT
            const response = await supabase.functions.invoke('speech-to-text', {
              body: { 
                audio: base64Audio,
                apiKey: openaiKey
              }
            });

      if (response.error) {
        console.error('TTS Response error:', response.error);
        throw new Error(`TTS API error: ${response.error.message || 'Unknown error'}`);
      }

            const transcript = response.data?.text;
            if (transcript) {
              setInputValue(transcript);
              toast({
                title: '–ì–æ–ª–æ—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω (—Å–µ—Ä–≤–µ—Ä)',
                description: `–¢–µ–∫—Å—Ç: "${transcript}"`
              });

              // Auto-send in voice mode
              setTimeout(() => {
                if (transcript.trim()) {
                  // Directly send the transcript without relying on inputValue state
                  const userMessage = transcript.trim();
                  setInputValue('');
                  
                  addMessage('user', userMessage, true);

                  // Add thinking indicator
                  const thinkingMessage: Message = {
                    id: 'thinking',
                    type: 'assistant',
                    content: '',
                    timestamp: new Date(),
                    thinking: true
                  };
                  setMessages(prev => [...prev, thinkingMessage]);

                  // Get AI response
                  generateResponse(userMessage).then(async (response) => {
                    setMessages(prev => {
                      const hasStreamingResponse = prev.some(m => m.id !== 'thinking' && m.type === 'assistant' && m.timestamp.getTime() > thinkingMessage.timestamp.getTime());
                      if (hasStreamingResponse) {
                        return prev.filter(m => m.id !== 'thinking');
                      } else {
                        return prev.filter(m => m.id !== 'thinking').concat([{
                          id: Date.now().toString(),
                          type: 'assistant',
                          content: response,
                          timestamp: new Date()
                        }]);
                      }
                    });
                    
                    if (isVoiceMode) {
                      await speakResponse(response);
                    }
                  }).catch(() => {
                    setMessages(prev => prev.filter(m => m.id !== 'thinking'));
                    addMessage('assistant', '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
                  });
                }
              }, 500);
            }
          };
          reader.readAsDataURL(audioBlob);
        } catch (error) {
          console.error('Server STT error:', error);
          toast({
            title: '–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è',
            description: '–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å',
            variant: 'destructive'
          });
        } finally {
          setVoiceState(prev => ({ ...prev, isListening: false }));
          stream.getTracks().forEach(track => track.stop());
        }
      };

      mediaRecorder.start();
      
      toast({
        title: '–ó–∞–ø–∏—Å—å...',
        description: '–ù–∞–∂–º–∏—Ç–µ –µ—â–µ —Ä–∞–∑ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏'
      });

      // Auto-stop after 10 seconds
      setTimeout(() => {
        if (mediaRecorder?.state === 'recording') {
          mediaRecorder.stop();
        }
      }, 10000);

    } catch (error) {
      console.error('Error with server STT:', error);
      setVoiceState(prev => ({ ...prev, isListening: false }));
      toast({
        title: '–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏',
        description: '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É',
        variant: 'destructive'
      });
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSendMessage();
    }
  };

  return (
    <div className="flex flex-col h-screen bg-background">
      {/* Header */}
      <div className="border-b bg-card p-4">
        <div className="max-w-4xl mx-auto flex items-center justify-between">
          <div className="flex items-center gap-3">
            <div className="w-10 h-10 rounded-full bg-primary flex items-center justify-center">
              <Bot className="w-5 h-5 text-primary-foreground" />
            </div>
            <div>
              <h1 className="text-lg font-semibold">–ì–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫</h1>
              <div className="text-sm text-muted-foreground">
                {voiceState.isSpeaking ? (
                  <span className="text-green-500">–ì–æ–≤–æ—Ä—é...</span>
                ) : voiceState.isListening ? (
                  <span className="text-blue-500">–°–ª—É—à–∞—é...</span>
                ) : voiceState.isConnected ? (
                  <span className="text-green-500">–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ</span>
                ) : (
                  '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ CRM —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å –∏ —Ç–µ–∫—Å—Ç'
                )}
              </div>
              
              {/* Status indicators for advanced features */}
              {userVoiceSettings && (
                <div className="flex flex-wrap gap-1 mt-1">
                  {userVoiceSettings?.advanced_features?.enable_function_calling !== false && (
                    <Badge variant="secondary" className="text-xs px-1 py-0">
                      üîß
                    </Badge>
                  )}
                  {userVoiceSettings?.advanced_features?.enable_memory !== false && (
                    <Badge variant="secondary" className="text-xs px-1 py-0">
                      üß†
                    </Badge>
                  )}
                  {userVoiceSettings?.advanced_features?.auto_save_conversations !== false && (
                    <Badge variant="secondary" className="text-xs px-1 py-0">
                      üíæ
                    </Badge>
                  )}
                  {userVoiceSettings?.advanced_features?.privacy_mode && (
                    <Badge variant="destructive" className="text-xs px-1 py-0">
                      üîí
                    </Badge>
                  )}
                  {userVoiceSettings?.ai_settings?.enable_streaming && (
                    <Badge variant="outline" className="text-xs px-1 py-0">
                      ‚ö°
                    </Badge>
                  )}
                </div>
              )}
            </div>
          </div>
          
          <div className="flex items-center gap-2">
            <Button
              variant={isVoiceMode ? "default" : "outline"}
              size="sm"
              onClick={toggleVoiceMode}
              className="gap-1"
            >
              {isVoiceMode ? <Volume2 className="h-4 w-4" /> : <VolumeX className="h-4 w-4" />}
              {isVoiceMode ? '–ì–æ–ª–æ—Å –í–ö–õ' : '–ì–æ–ª–æ—Å –í–´–ö–õ'}
            </Button>
            <Button variant="outline" size="sm" onClick={clearChat}>
              <Trash2 className="h-4 w-4" />
            </Button>
          </div>
        </div>
      </div>

      {/* Browser support warning */}
      {(!browserSupport.mediaDevices || !browserSupport.speechSynthesis) && (
        <div className="bg-yellow-50 border-b border-yellow-200 p-3">
          <div className="max-w-4xl mx-auto flex items-center gap-2 text-yellow-800">
            <AlertCircle className="w-4 h-4" />
            <span className="text-sm">
              –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ—Å–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å.
            </span>
          </div>
        </div>
      )}

      {/* Messages */}
      <ScrollArea className="flex-1 p-4">
        <div className="space-y-4 max-w-4xl mx-auto">
          {messages.map((message) => (
            <div key={message.id} className="flex items-start gap-3">
              <div className={cn(
                "flex-shrink-0 w-8 h-8 rounded-full flex items-center justify-center",
                message.type === 'assistant' 
                  ? "bg-primary text-primary-foreground" 
                  : "bg-muted text-muted-foreground"
              )}>
                {message.type === 'assistant' ? (
                  <Bot className="w-4 h-4" />
                ) : (
                  <User className="w-4 h-4" />
                )}
              </div>
              <div className="flex-1 space-y-2">
                <div className="flex items-center gap-2">
                  <span className="text-sm font-medium">
                    {message.type === 'assistant' ? '–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç' : '–í—ã'}
                  </span>
                  {message.isVoice && (
                    <Badge variant="secondary" className="text-xs">
                      –ì–æ–ª–æ—Å
                    </Badge>
                  )}
                  <span className="text-xs text-muted-foreground">
                    {message.timestamp.toLocaleTimeString()}
                  </span>
                </div>
                {message.thinking ? (
                  <div className="flex items-center gap-2">
                    <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-primary"></div>
                    <span className="text-sm text-muted-foreground">–î—É–º–∞—é...</span>
                  </div>
                ) : (
                  <div className="bg-muted rounded-lg p-3">
                    <div className="text-sm whitespace-pre-wrap">{message.content}</div>
                  </div>
                )}
              </div>
            </div>
          ))}
          <div ref={messagesEndRef} />
        </div>
      </ScrollArea>

      {/* Input */}
      <div className="border-t bg-card p-4">
        <div className="max-w-4xl mx-auto">
          <div className="flex items-end gap-2">
            <div className="flex-1">
              <Input
                value={inputValue}
                onChange={(e) => setInputValue(e.target.value)}
                onKeyPress={handleKeyPress}
                placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å –æ CRM..."
                className="min-h-[44px]"
              />
            </div>
            <Button
              onClick={handleVoiceInput}
              variant="outline"
              size="icon"
              className={cn(
                "w-11 h-11",
                voiceState.isListening && "bg-red-500 text-white hover:bg-red-600"
              )}
              disabled={!browserSupport.mediaDevices}
            >
              {voiceState.isListening ? (
                <MicOff className="w-4 h-4" />
              ) : (
                <Mic className="w-4 h-4" />
              )}
            </Button>
            <Button
              onClick={handleSendMessage}
              disabled={!inputValue.trim()}
              size="icon"
              className="w-11 h-11"
            >
              <Send className="w-4 h-4" />
            </Button>
          </div>
          
          {/* Quick action buttons */}
          <div className="flex flex-wrap gap-2 mt-3">
            <Button
              variant="outline"
              size="sm"
              onClick={() => setInputValue("–ü–æ–∫–∞–∂–∏ –º–æ–∏ –∑–∞–¥–∞—á–∏")}
              className="text-xs"
            >
              –ú–æ–∏ –∑–∞–¥–∞—á–∏
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={() => setInputValue("–ö–∞–∫–∏–µ –∫–ª–∏–µ–Ω—Ç—ã –≤ —Ä–∞–±–æ—Ç–µ?")}
              className="text-xs"
            >
              –ö–ª–∏–µ–Ω—Ç—ã –≤ —Ä–∞–±–æ—Ç–µ
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={() => setInputValue("–°–æ–∑–¥–∞–π —Å–º–µ—Ç—É –Ω–∞ –≥–∞–∑–æ–Ω 100 –∫–≤.–º")}
              className="text-xs"
            >
              –°–æ–∑–¥–∞—Ç—å —Å–º–µ—Ç—É
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={() => setInputValue("–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É: —Å–≤—è–∑–∞—Ç—å—Å—è —Å –∫–ª–∏–µ–Ω—Ç–æ–º")}
              className="text-xs"
            >
              –°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={() => setInputValue("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ—Ä–æ–Ω–∫—É –ø—Ä–æ–¥–∞–∂")}
              className="text-xs"
            >
              –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={() => setInputValue("–°–æ—Å—Ç–∞–≤—å —Å–º–µ—Ç—É –Ω–∞ –¥—Ä–µ–Ω–∞–∂ 50 –º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –ò–≤–∞–Ω–æ–≤–∞")}
              className="text-xs"
            >
              –°–º–µ—Ç–∞ —Å –∫–ª–∏–µ–Ω—Ç–æ–º
            </Button>
          </div>
        </div>
      </div>
    </div>
  );
};

export default VoiceChatAssistant;
import React, { useState, useRef, useEffect, useCallback } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { ScrollArea } from '@/components/ui/scroll-area';
import { useToast } from '@/components/ui/use-toast';
import { 
  Mic, 
  MicOff, 
  Send, 
  Volume2, 
  VolumeX, 
  Bot, 
  User,
  Settings,
  Trash2,
  Copy,
  MoreVertical
} from 'lucide-react';
import { cn } from '@/lib/utils';
import { supabase } from '@/integrations/supabase/client';

interface Message {
  id: string;
  type: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  isVoice?: boolean;
  thinking?: boolean;
}

interface VoiceState {
  isListening: boolean;
  isSpeaking: boolean;
  isConnected: boolean;
  volume: number;
}

const VoiceChatAssistant = () => {
  const { toast } = useToast();
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      type: 'assistant',
      content: '–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à –≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è. –ú–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö, —Å–º–µ—Ç–∞—Ö, –∑–∞–¥–∞—á–∞—Ö –∏ –ø–æ–º–æ—á—å —É–ø—Ä–∞–≤–ª—è—Ç—å –¥—Ä—É–≥–∏–º–∏ –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫–∞–º–∏. –°–ø—Ä–∞—à–∏–≤–∞–π—Ç–µ –≥–æ–ª–æ—Å–æ–º –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–º!',
      timestamp: new Date(),
    }
  ]);
  const [inputValue, setInputValue] = useState('');
  const [voiceState, setVoiceState] = useState<VoiceState>({
    isListening: false,
    isSpeaking: false,
    isConnected: false,
    volume: 0.8
  });
  const [isVoiceMode, setIsVoiceMode] = useState(false);
  
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const scrollAreaRef = useRef<HTMLDivElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  // Scroll to bottom when new messages arrive
  const scrollToBottom = useCallback(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, []);

  useEffect(() => {
    scrollToBottom();
  }, [messages, scrollToBottom]);

  // Add message helper
  const addMessage = useCallback((type: 'user' | 'assistant', content: string, isVoice = false) => {
    const newMessage: Message = {
      id: Date.now().toString(),
      type,
      content,
      timestamp: new Date(),
      isVoice
    };
    setMessages(prev => [...prev, newMessage]);
  }, []);

  // Send text message
  const handleSendMessage = useCallback(async () => {
    if (!inputValue.trim()) return;

    const userMessage = inputValue.trim();
    setInputValue('');
    
    addMessage('user', userMessage);

    // Add thinking indicator
    const thinkingMessage: Message = {
      id: 'thinking',
      type: 'assistant',
      content: '',
      timestamp: new Date(),
      thinking: true
    };
    setMessages(prev => [...prev, thinkingMessage]);

    // Get AI response
    try {
      const response = await generateResponse(userMessage);
      setMessages(prev => prev.filter(m => m.id !== 'thinking'));
      addMessage('assistant', response);
      
      // If voice mode is enabled, speak the response
      if (isVoiceMode) {
        speakResponse(response);
      }
    } catch (error) {
      setMessages(prev => prev.filter(m => m.id !== 'thinking'));
      
      const errorMessage = error.message.includes('OpenAI') 
        ? '–ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.'
        : error.message.includes('network') || error.message.includes('fetch')
        ? '–ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.'
        : '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.';
      
      addMessage('assistant', errorMessage);
    }
  }, [inputValue, addMessage, isVoiceMode, voiceState.volume]);

  // Generate AI response using enhanced voice chat system
  const generateResponse = async (userMessage: string): Promise<string> => {
    try {
      const { data, error } = await supabase.functions.invoke('enhanced-voice-chat', {
        body: { 
          message: userMessage, 
          conversation_history: messages.slice(-10).map(m => ({
            type: m.type,
            content: m.content
          }))
        }
      });

      if (error) {
        console.error('Error calling enhanced-voice-chat function:', error);
        return '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.';
      }

      return data.response || '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫–∞.';
    } catch (error) {
      console.error('Error in generateResponse:', error);
      return '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫—É.';
    }
  };

  // Text-to-speech helper function
  const speakResponse = useCallback((text: string) => {
    if (!isVoiceMode) {
      console.log('Voice mode is disabled, skipping speech');
      return;
    }
    
    // Stop any current speech
    speechSynthesis.cancel();
    
    console.log('Starting speech synthesis for:', text.substring(0, 100) + '...');
    
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.rate = 0.9;
    utterance.pitch = 1;
    utterance.volume = voiceState.volume;
    utterance.lang = 'ru-RU';
    
    utterance.onstart = () => {
      console.log('Speech synthesis started');
      setVoiceState(prev => ({ ...prev, isSpeaking: true }));
    };
    
    utterance.onend = () => {
      console.log('Speech synthesis ended');
      setVoiceState(prev => ({ ...prev, isSpeaking: false }));
    };
    
    utterance.onerror = (event) => {
      console.error('Speech synthesis error:', event.error);
      setVoiceState(prev => ({ ...prev, isSpeaking: false }));
      toast({
        title: '–û—à–∏–±–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è',
        description: '–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –æ—Ç–≤–µ—Ç –≥–æ–ª–æ—Å–æ–º',
        variant: 'destructive'
      });
    };
    
    // Ensure voices are loaded
    const voices = speechSynthesis.getVoices();
    if (voices.length === 0) {
      // Wait for voices to load
      speechSynthesis.addEventListener('voiceschanged', () => {
        const russianVoice = speechSynthesis.getVoices().find(voice => 
          voice.lang.startsWith('ru') || voice.lang.includes('RU')
        );
        if (russianVoice) {
          utterance.voice = russianVoice;
        }
        speechSynthesis.speak(utterance);
      }, { once: true });
    } else {
      // Find Russian voice
      const russianVoice = voices.find(voice => 
        voice.lang.startsWith('ru') || voice.lang.includes('RU')
      );
      if (russianVoice) {
        utterance.voice = russianVoice;
        console.log('Using Russian voice:', russianVoice.name);
      } else {
        console.log('Russian voice not found, using default');
      }
      
      speechSynthesis.speak(utterance);
    }
  }, [isVoiceMode, voiceState.volume, toast]);

  // Cleanup function for stopping recording and releasing resources
  const cleanupRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // Stop any ongoing speech synthesis
    speechSynthesis.cancel();
    
    mediaRecorderRef.current = null;
    audioChunksRef.current = [];
    setVoiceState(prev => ({ ...prev, isListening: false, isSpeaking: false }));
    setMessages(prev => prev.filter(m => m.content !== 'üé§ –°–ª—É—à–∞—é...'));
  }, []);

  // Process recorded audio and send to speech-to-text
  const processRecordedAudio = useCallback(async () => {
    console.log('Processing recorded audio...');
    
    if (audioChunksRef.current.length === 0) {
      console.warn('No audio data recorded');
      cleanupRecording();
      return;
    }
    
    try {
      // Convert audio to base64
      const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
      const arrayBuffer = await audioBlob.arrayBuffer();
      const uint8Array = new Uint8Array(arrayBuffer);
      
      // Convert to base64 in chunks to prevent memory issues
      let binary = '';
      const chunkSize = 0x8000;
      for (let i = 0; i < uint8Array.length; i += chunkSize) {
        const chunk = uint8Array.subarray(i, Math.min(i + chunkSize, uint8Array.length));
        binary += String.fromCharCode.apply(null, Array.from(chunk));
      }
      const base64Audio = btoa(binary);
      
      console.log('Sending audio to speech-to-text...');
      
      // Send to speech-to-text function
      const { data: transcriptionData, error: transcriptionError } = await supabase.functions.invoke('speech-to-text', {
        body: { audio: base64Audio }
      });
      
      setVoiceState(prev => ({ ...prev, isListening: false }));
      setMessages(prev => prev.filter(m => m.content !== 'üé§ –°–ª—É—à–∞—é...'));
      
      if (transcriptionError) {
        console.error('Transcription error:', transcriptionError);
        const errorMessage = transcriptionError.message.includes('OpenAI') 
          ? '–°–µ—Ä–≤–∏—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.'
          : '–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∏ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.';
        addMessage('assistant', errorMessage);
        return;
      }
      
      const transcript = transcriptionData.text || '';
      console.log('Transcription result:', transcript);
      
      if (!transcript.trim()) {
        addMessage('assistant', '–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å –≥—Ä–æ–º—á–µ –∏ —á–µ—Ç—á–µ.');
        return;
      }
      
      addMessage('user', transcript, true);
      
      // Add thinking indicator
      const thinkingMessage: Message = {
        id: 'thinking',
        type: 'assistant',
        content: '',
        timestamp: new Date(),
        thinking: true
      };
      setMessages(prev => [...prev, thinkingMessage]);
      
      // Process voice message with enhanced system
      try {
        const response = await generateResponse(transcript);
        setMessages(prev => prev.filter(m => m.id !== 'thinking'));
        addMessage('assistant', response);
        
        // Save to command history
        await supabase.functions.invoke('voice-chat', {
          body: { 
            message: `create_command_history: ${JSON.stringify({
              voice_text: transcript,
              transcript: transcript,
              actions: ['voice_processing'],
              execution_result: { response }
            })}` 
          }
        });
        
        if (isVoiceMode) {
          speakResponse(response);
        }
      } catch (error) {
        console.error('Error processing voice message:', error);
        setMessages(prev => prev.filter(m => m.id !== 'thinking'));
        
        const errorMessage = error.message.includes('OpenAI') 
          ? '–ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.'
          : error.message.includes('network') || error.message.includes('fetch')
          ? '–ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.'
          : '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–º–∞–Ω–¥—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.';
        
        addMessage('assistant', errorMessage);
      }
      
    } catch (error) {
      console.error('Error processing audio:', error);
      setVoiceState(prev => ({ ...prev, isListening: false }));
      setMessages(prev => prev.filter(m => m.content !== 'üé§ –°–ª—É—à–∞—é...'));
      
      const errorMessage = error.message.includes('QuotaExceededError') 
        ? '–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞. –ó–∞–ø–∏—à–∏—Ç–µ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.'
        : error.message.includes('network') || error.message.includes('fetch')
        ? '–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—É–¥–∏–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ.'
        : '–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.';
      
      addMessage('assistant', errorMessage);
    }
  }, [addMessage, cleanupRecording, generateResponse, isVoiceMode]);

  // Start voice recording
  const startVoiceRecording = useCallback(async () => {
    // Prevent multiple recordings
    if (voiceState.isListening || mediaRecorderRef.current) {
      console.log('Recording already in progress');
      return;
    }

    // Stop any ongoing speech synthesis when starting to record
    speechSynthesis.cancel();
    setVoiceState(prev => ({ ...prev, isSpeaking: false }));

    try {
      console.log('Starting voice recording...');
      
      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      streamRef.current = stream;
      setVoiceState(prev => ({ ...prev, isListening: true, isConnected: true }));
      addMessage('user', 'üé§ –°–ª—É—à–∞—é...', true);
      
      // Initialize MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm; codecs=opus'
      });
      
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };
      
      mediaRecorder.onstop = async () => {
        console.log('Recording stopped, processing audio...');
        await processRecordedAudio();
      };
      
      mediaRecorder.start(1000); // Record in 1 second chunks
      console.log('MediaRecorder started');
      
    } catch (error) {
      console.error('Error starting voice recording:', error);
      cleanupRecording();
      toast({
        title: '–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É',
        description: '–†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞',
        variant: 'destructive'
      });
    }
  }, [voiceState.isListening, addMessage, toast, cleanupRecording, processRecordedAudio]);

  // Stop voice recording
  const stopVoiceRecording = useCallback(() => {
    console.log('Stopping voice recording...');
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      cleanupRecording();
    };
  }, [cleanupRecording]);

  const toggleVoiceMode = useCallback(() => {
    const newVoiceMode = !isVoiceMode;
    setIsVoiceMode(newVoiceMode);
    
    if (newVoiceMode) {
      toast({
        title: '–ì–æ–ª–æ—Å–æ–≤–æ–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω',
        description: '–¢–µ–ø–µ—Ä—å –ø–æ–º–æ—â–Ω–∏–∫ –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–º'
      });
      
      // Test speech synthesis
      setTimeout(() => {
        if (newVoiceMode) { // Check again in case it was toggled quickly
          const utterance = new SpeechSynthesisUtterance('–ì–æ–ª–æ—Å–æ–≤–æ–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω. –Ø –≥–æ—Ç–æ–≤ –æ—Ç–≤–µ—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–º!');
          utterance.rate = 0.9;
          utterance.pitch = 1;
          utterance.volume = voiceState.volume;
          utterance.lang = 'ru-RU';
          speechSynthesis.speak(utterance);
        }
      }, 500);
    } else {
      // Stop any ongoing speech when disabling voice mode
      speechSynthesis.cancel();
      setVoiceState(prev => ({ ...prev, isSpeaking: false }));
      
      toast({
        title: '–ì–æ–ª–æ—Å–æ–≤–æ–π —Ä–µ–∂–∏–º –≤—ã–∫–ª—é—á–µ–Ω',
        description: '–ü–æ–º–æ—â–Ω–∏–∫ –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–º'
      });
    }
  }, [isVoiceMode, toast, voiceState.volume]);

  const clearChat = useCallback(() => {
    setMessages([{
      id: '1',
      type: 'assistant',
      content: '–ß–∞—Ç –æ—á–∏—â–µ–Ω. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?',
      timestamp: new Date(),
    }]);
  }, []);

  // Get command history
  const [commandHistory, setCommandHistory] = useState<any[]>([]);
  
  const loadCommandHistory = useCallback(async () => {
    try {
      const { data, error } = await supabase.functions.invoke('voice-chat', {
        body: { message: 'get_command_history' }
      });
      
      if (!error && data) {
        setCommandHistory(data.history || []);
      }
    } catch (error) {
      console.error('Error loading command history:', error);
    }
  }, []);

  useEffect(() => {
    loadCommandHistory();
  }, [loadCommandHistory]);

  return (
    <div className="h-full flex flex-col bg-gradient-to-br from-background to-muted/20">
      {/* Header */}
      <div className="flex items-center justify-between p-4 border-b bg-card/50 backdrop-blur-sm">
        <div className="flex items-center gap-3">
          <div className="h-10 w-10 rounded-lg bg-gradient-to-br from-primary to-primary/70 flex items-center justify-center">
            <Bot className="h-5 w-5 text-primary-foreground" />
          </div>
          <div>
            <h1 className="text-lg font-semibold">–ì–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è</h1>
            <p className="text-sm text-muted-foreground">
              {voiceState.isListening ? (
                <span className="text-green-500 flex items-center gap-1">
                  <div className="h-2 w-2 bg-green-500 rounded-full animate-pulse" />
                  –°–ª—É—à–∞—é...
                </span>
              ) : voiceState.isSpeaking ? (
                <span className="text-blue-500 flex items-center gap-1">
                  <div className="h-2 w-2 bg-blue-500 rounded-full animate-pulse" />
                  –ì–æ–≤–æ—Ä—é...
                </span>
              ) : voiceState.isConnected ? (
                <span className="text-green-500">–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ</span>
              ) : (
                '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ CRM —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å –∏ —Ç–µ–∫—Å—Ç'
              )}
            </p>
          </div>
        </div>
        
        <div className="flex items-center gap-2">
          <Button
            variant={isVoiceMode ? "default" : "outline"}
            size="sm"
            onClick={toggleVoiceMode}
            className="gap-1"
          >
            {isVoiceMode ? <Volume2 className="h-4 w-4" /> : <VolumeX className="h-4 w-4" />}
            {isVoiceMode ? '–ì–æ–ª–æ—Å –í–ö–õ' : '–ì–æ–ª–æ—Å –í–´–ö–õ'}
          </Button>
          {voiceState.isSpeaking && (
            <Button
              variant="outline"
              size="sm"
              onClick={() => {
                speechSynthesis.cancel();
                setVoiceState(prev => ({ ...prev, isSpeaking: false }));
              }}
              className="gap-1"
            >
              <VolumeX className="h-4 w-4" />
              –°—Ç–æ–ø
            </Button>
          )}
          <Button variant="outline" size="sm" onClick={clearChat}>
            <Trash2 className="h-4 w-4" />
          </Button>
        </div>
      </div>

      {/* Messages */}
      <ScrollArea ref={scrollAreaRef} className="flex-1 p-4">
        <div className="space-y-4 max-w-4xl mx-auto">
          {messages.map((message) => (
            <div
              key={message.id}
              className={cn(
                "flex gap-3",
                message.type === 'user' ? 'justify-end' : 'justify-start'
              )}
            >
              {message.type === 'assistant' && (
                <div className="h-8 w-8 rounded-full bg-primary flex items-center justify-center flex-shrink-0">
                  <Bot className="h-4 w-4 text-primary-foreground" />
                </div>
              )}
              
              <div
                className={cn(
                  "max-w-[80%] rounded-lg px-4 py-2 relative group",
                  message.type === 'user'
                    ? "bg-primary text-primary-foreground ml-12"
                    : "bg-card border shadow-sm"
                )}
              >
                {message.thinking ? (
                  <div className="flex items-center gap-2 text-muted-foreground">
                    <div className="flex gap-1">
                      <div className="h-2 w-2 bg-current rounded-full animate-bounce" style={{ animationDelay: '0ms' }} />
                      <div className="h-2 w-2 bg-current rounded-full animate-bounce" style={{ animationDelay: '150ms' }} />
                      <div className="h-2 w-2 bg-current rounded-full animate-bounce" style={{ animationDelay: '300ms' }} />
                    </div>
                    <span className="text-sm">–î—É–º–∞—é...</span>
                  </div>
                ) : (
                  <div>
                    <div className="text-sm leading-relaxed">{message.content}</div>
                    <div className="flex items-center justify-between mt-2">
                      <span className="text-xs opacity-70">
                        {message.timestamp.toLocaleTimeString('ru-RU', { hour: '2-digit', minute: '2-digit' })}
                      </span>
                      {message.isVoice && (
                        <Badge variant="secondary" className="text-xs">
                          <Mic className="h-3 w-3 mr-1" />
                          –ì–æ–ª–æ—Å
                        </Badge>
                      )}
                    </div>
                  </div>
                )}
              </div>
              
              {message.type === 'user' && (
                <div className="h-8 w-8 rounded-full bg-secondary flex items-center justify-center flex-shrink-0">
                  <User className="h-4 w-4" />
                </div>
              )}
            </div>
          ))}
          <div ref={messagesEndRef} />
        </div>
      </ScrollArea>

      {/* Input */}
      <div className="border-t bg-card/50 backdrop-blur-sm p-4">
        <div className="max-w-4xl mx-auto">
          <div className="flex gap-2">
            <div className="flex-1 relative">
              <Input
                value={inputValue}
                onChange={(e) => setInputValue(e.target.value)}
                placeholder="–°–ø—Ä–æ—Å–∏—Ç–µ –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö, —Å–º–µ—Ç–∞—Ö, –∑–∞–¥–∞—á–∞—Ö –∏–ª–∏ –¥–∞–π—Ç–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ..."
                onKeyPress={(e) => e.key === 'Enter' && !e.shiftKey && handleSendMessage()}
                className="pr-12"
              />
              <Button
                size="sm"
                variant="ghost"
                className="absolute right-1 top-1/2 -translate-y-1/2 h-8 w-8 p-0"
                onClick={handleSendMessage}
                disabled={!inputValue.trim()}
              >
                <Send className="h-4 w-4" />
              </Button>
            </div>
            
            <Button
              size="default"
              variant={voiceState.isListening ? "destructive" : "default"}
              className={cn(
                "h-10 w-10 p-0 relative",
                voiceState.isListening && "animate-pulse"
              )}
              onClick={voiceState.isListening ? stopVoiceRecording : startVoiceRecording}
            >
              {voiceState.isListening ? (
                <MicOff className="h-5 w-5" />
              ) : (
                <Mic className="h-5 w-5" />
              )}
              {voiceState.isListening && (
                <div className="absolute inset-0 rounded-md bg-destructive/20 animate-ping" />
              )}
            </Button>
          </div>
          
          <p className="text-xs text-muted-foreground mt-2 text-center">
            –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –¥–ª—è –Ω–∞—á–∞–ª–∞/–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ –≥–æ–ª–æ—Å–∞
          </p>
          
          {/* Command History */}
          {commandHistory.length > 0 && (
            <div className="mt-4 p-3 bg-muted/50 rounded-lg">
              <h4 className="text-sm font-medium mb-2">–ü–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã:</h4>
              <div className="space-y-1 max-h-32 overflow-y-auto">
                {commandHistory.slice(0, 5).map((cmd: any) => (
                  <div key={cmd.id} className="text-xs text-muted-foreground p-2 bg-background rounded border-l-2 border-primary/20">
                    <span className="font-medium block">{cmd.transcript}</span>
                    <span className="text-xs opacity-60 block">
                      {new Date(cmd.created_at).toLocaleString('ru-RU')} ‚Ä¢ {cmd.status}
                    </span>
                  </div>
                ))}
              </div>
            </div>
          )}
        </div>
      </div>
    </div>
  );
};

export default VoiceChatAssistant;